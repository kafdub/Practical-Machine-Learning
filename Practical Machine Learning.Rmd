---
title: "Practical Machine Learning"
author: "Liang Pei Ling"
date: "11/12/2017"
output: html_document
---

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ??? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants will be used to predict the manner in which they did the exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Loading data and packages for analysis

```{r cars}

library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(knitr)


training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

```

Partioning the training set into two and cleaning the data.

```{r}

inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)

```

## Cleaning the data

```{r}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

myTraining <- myTraining[c(-1)]

## Clean variables with more than 60% NA

trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)

```


Transform the myTesting and testing data sets

```{r}
clean1 <- colnames(myTraining)

# remove the classe column
clean2 <- colnames(myTraining[, -58])  

 # allow only variables in myTesting that are also in myTraining
myTesting <- myTesting[clean1]        

# allow only variables in testing that are also in myTraining
testing <- testing[clean2]             


# Coerce the data into the same type

for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]

```
## Model Building - Random Forest Model

Random Forest Model was chosen due to its known good performance and prediction. I fit the model on ptrain1, and used the ???train??? function to use 3-fold cross-validation to select optimal tuning parameters for the model.


```{r}

set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```

## Model Evaluation & Selection

The accuracy of Random Forest is 99.8%, thus my predicted accuracy for the out-of-sample error is 0.2%.

This is an excellent result, so rather than trying additional algorithms, I will use Random Forests to predict on the test set.

## Predicting Results on Test Data

```{r}

predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2

```



